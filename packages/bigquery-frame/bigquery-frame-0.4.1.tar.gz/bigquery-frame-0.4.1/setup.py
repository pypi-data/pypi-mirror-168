# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['bigquery_frame',
 'bigquery_frame.data_diff',
 'bigquery_frame.transformations_impl']

package_data = \
{'': ['*']}

install_requires = \
['google-cloud-bigquery-storage>=2.14.1,<3.0.0',
 'google-cloud-bigquery>=3.3.1,<4.0.0',
 'tabulate>=0.8.10,<0.9.0',
 'tqdm>=4.64.0,<5.0.0']

setup_kwargs = {
    'name': 'bigquery-frame',
    'version': '0.4.1',
    'description': 'A DataFrame API for Google BigQuery',
    'long_description': '# Bigquery-frame\n\n[![PyPI version](https://badge.fury.io/py/bigquery-frame.svg)](https://badge.fury.io/py/bigquery-frame)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/bigquery-frame.svg)](https://pypi.org/project/bigquery-frame/)\n[![GitHub Build](https://img.shields.io/github/workflow/status/FurcyPin/bigquery-frame/Build%20and%20Validate)](https://github.com/FurcyPin/bigquery-frame/actions)\n[![SonarCloud Coverage](https://sonarcloud.io/api/project_badges/measure?project=FurcyPin_bigquery-frame&metric=coverage)](https://sonarcloud.io/component_measures?id=FurcyPin_bigquery-frame&metric=coverage&view=list)\n[![SonarCloud Bugs](https://sonarcloud.io/api/project_badges/measure?project=FurcyPin_bigquery-frame&metric=bugs)](https://sonarcloud.io/component_measures?metric=reliability_rating&view=list&id=FurcyPin_bigquery-frame)\n[![SonarCloud Vulnerabilities](https://sonarcloud.io/api/project_badges/measure?project=FurcyPin_bigquery-frame&metric=vulnerabilities)](https://sonarcloud.io/component_measures?metric=security_rating&view=list&id=FurcyPin_bigquery-frame)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/bigquery-frame)](https://pypi.org/project/bigquery-frame/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n## What is it ?\n\nThis project is a POC that aims to showcase the wonders that could be done if BigQuery provided a DataFrame API in \nPython similar to the one already available with PySpark or Snowpark (for which the Python API will come out soon).\n\nI tried to reproduce the most commonly used methods of the Spark DataFrame object. I aimed at making something \nas close as possible as PySpark, and tried to keep exactly the same naming and docstrings as PySpark\'s DataFrames.\n \n\nFor instance, this is a working example of PySpark code:\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as f\n\nspark = SparkSession.builder.master("local[1]").getOrCreate()\n\ndf = spark.sql("""\n    SELECT 1 as id, "Bulbasaur" as name, ARRAY("Grass", "Poison") as types, NULL as other_col\n    UNION ALL\n    SELECT 2 as id, "Ivysaur" as name, ARRAY("Grass", "Poison") as types, NULL as other_col\n""")\n\ndf.select("id", "name", "types").createOrReplaceTempView("pokedex")\n\ndf2 = spark.sql("""SELECT * FROM pokedex""")\\\n    .withColumn("nb_types", f.expr("SIZE(types)"))\\\n    .withColumn("name", f.expr("LOWER(name)"))\n\ndf2.show()\n# +---+---------+---------------+--------+\n# | id|     name|          types|nb_types|\n# +---+---------+---------------+--------+\n# |  1|bulbasaur|[Grass, Poison]|       2|\n# |  2|  ivysaur|[Grass, Poison]|       2|\n# +---+---------+---------------+--------+\n```\n\nAnd this is an equivalent working example using bigquery_frame, that runs on Google Big Query! \n```python\nfrom bigquery_frame import BigQueryBuilder\nfrom bigquery_frame.auth import get_bq_client\nfrom bigquery_frame import functions as f\n\nbigquery = BigQueryBuilder(get_bq_client())\n\ndf = bigquery.sql("""\n    SELECT 1 as id, "Bulbasaur" as name, ["Grass", "Poison"] as types, NULL as other_col\n    UNION ALL\n    SELECT 2 as id, "Ivysaur" as name, ["Grass", "Poison"] as types, NULL as other_col\n""")\n\ndf.select("id", "name", "types").createOrReplaceTempView("pokedex")\n\ndf2 = bigquery.sql("""SELECT * FROM pokedex""")\\\n    .withColumn("nb_types", f.expr("ARRAY_LENGTH(types)"))\\\n    .withColumn("name", f.expr("LOWER(name)"), replace=True)\n\ndf2.show()\n# +----+-----------+---------------------+----------+\n# | id |      name |               types | nb_types |\n# +----+-----------+---------------------+----------+\n# |  1 | bulbasaur | [\'Grass\', \'Poison\'] |        2 |\n# |  2 |   ivysaur | [\'Grass\', \'Poison\'] |        2 |\n# +----+-----------+---------------------+----------+\n```\n\n## What\'s so cool about DataFrames ?\n\nI believe that DataFrames are super cool to organise SQL code as it allows us to \nseveral things that are much harder, or even impossible, in pure-SQL:\n\n- on-the-fly introspection\n- chaining operations\n- generic transformations\n- higher level abstraction\n\nBut that deserves [a blog article](https://towardsdatascience.com/sql-jinja-is-not-enough-why-we-need-dataframes-4d71a191936d).\n\n\n## Cool Features\n\nThis feature list is arbitrarily sorted by decreasing order of coolness.\n\n### Data Diff _(NEW!)_\n_Just like git diff, but for data!_\n\nPerforms a diff between two dataframes.\n\n**Example:**\n```python\nfrom bigquery_frame.data_diff import DataframeComparator\nfrom bigquery_frame import BigQueryBuilder\nfrom bigquery_frame.auth import get_bq_client\n\nbq = BigQueryBuilder(get_bq_client())\ndf1 = bq.sql("""\n    SELECT * FROM UNNEST ([\n        STRUCT(1 as id, [STRUCT(1 as a, 2 as b, 3 as c)] as my_array),\n        STRUCT(2 as id, [STRUCT(1 as a, 2 as b, 3 as c)] as my_array),\n        STRUCT(3 as id, [STRUCT(1 as a, 2 as b, 3 as c)] as my_array)\n    ])\n""")\ndf2 = bq.sql("""\n    SELECT * FROM UNNEST ([\n        STRUCT(1 as id, [STRUCT(1 as a, 2 as b, 3 as c, 4 as d)] as my_array),\n        STRUCT(2 as id, [STRUCT(2 as a, 2 as b, 3 as c, 4 as d)] as my_array),\n        STRUCT(4 as id, [STRUCT(1 as a, 2 as b, 3 as c, 4 as d)] as my_array)\n   ])\n""")\n\ndf1.show()\n# +----+----------------------------+\n# | id |                   my_array |\n# +----+----------------------------+\n# |  1 | [{\'a\': 1, \'b\': 2, \'c\': 3}] |\n# |  2 | [{\'a\': 1, \'b\': 2, \'c\': 3}] |\n# |  3 | [{\'a\': 1, \'b\': 2, \'c\': 3}] |\n# +----+----------------------------+\ndf2.show()\n# +----+------------------------------------+\n# | id |                           my_array |\n# +----+------------------------------------+\n# |  1 | [{\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4}] |\n# |  2 | [{\'a\': 2, \'b\': 2, \'c\': 3, \'d\': 4}] |\n# |  4 | [{\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4}] |\n# +----+------------------------------------+\ndiff_result = DataframeComparator().compare_df(df1, df2)\ndiff_result.display()\n```\n\nWill produce the following output:\n\n```\nSchema has changed:\n@@ -2,3 +2,4 @@\n my_array!.a INTEGER\n my_array!.b INTEGER\n my_array!.c INTEGER\n+my_array!.d INTEGER\nWARNING: columns that do not match both sides will be ignored\ndiff NOT ok\nSummary:\nRow count ok: 3 rows\n1 (25.0%) rows are identical\n1 (25.0%) rows have changed\n1 (25.0%) rows are only in \'left\'\n1 (25.0%) rows are only in \'right\n100%|██████████| 1/1 [00:04<00:00,  4.26s/it]\nFound the following differences:\n+-------------+---------------+-----------------------+-----------------------+----------------+\n| column_name | total_nb_diff |                  left |                 right | nb_differences |\n+-------------+---------------+-----------------------+-----------------------+----------------+\n|    my_array |             1 | [{"a":1,"b":2,"c":3}] | [{"a":2,"b":2,"c":3}] |              1 |\n+-------------+---------------+-----------------------+-----------------------+----------------+\n1 rows were only found in \'left\' :\nAnalyzing 4 columns ...\n+---------------+-------------+-------------+-------+----------------+------------+-----+-----+------------------------------+\n| column_number | column_name | column_type | count | count_distinct | count_null | min | max |               approx_top_100 |\n+---------------+-------------+-------------+-------+----------------+------------+-----+-----+------------------------------+\n|             0 |          id |     INTEGER |     1 |              1 |          0 |   3 |   3 | [{\'value\': \'3\', \'count\': 1}] |\n|             1 | my_array!.a |     INTEGER |     1 |              1 |          0 |   1 |   1 | [{\'value\': \'1\', \'count\': 1}] |\n|             2 | my_array!.b |     INTEGER |     1 |              1 |          0 |   2 |   2 | [{\'value\': \'2\', \'count\': 1}] |\n|             3 | my_array!.c |     INTEGER |     1 |              1 |          0 |   3 |   3 | [{\'value\': \'3\', \'count\': 1}] |\n+---------------+-------------+-------------+-------+----------------+------------+-----+-----+------------------------------+\n1 rows were only found in \'right\':\nAnalyzing 4 columns ...\n+---------------+-------------+-------------+-------+----------------+------------+-----+-----+------------------------------+\n| column_number | column_name | column_type | count | count_distinct | count_null | min | max |               approx_top_100 |\n+---------------+-------------+-------------+-------+----------------+------------+-----+-----+------------------------------+\n|             0 |          id |     INTEGER |     1 |              1 |          0 |   4 |   4 | [{\'value\': \'4\', \'count\': 1}] |\n|             1 | my_array!.a |     INTEGER |     1 |              1 |          0 |   1 |   1 | [{\'value\': \'1\', \'count\': 1}] |\n|             2 | my_array!.b |     INTEGER |     1 |              1 |          0 |   2 |   2 | [{\'value\': \'2\', \'count\': 1}] |\n|             3 | my_array!.c |     INTEGER |     1 |              1 |          0 |   3 |   3 | [{\'value\': \'3\', \'count\': 1}] |\n+---------------+-------------+-------------+-------+----------------+------------+-----+-----+------------------------------+\n```\n\n- Full support of nested records\n- (improvable) support for repeated records\n- Optimized to work even on huge table with more than 1000 columns\n\n### Analyze\n\nPerform an analysis on a DataFrame, return aggregated stats for each column,\nsuch as count, count distinct, count null, min, max, top 100 most frequent values. \n\n- Optimized to work on wide tables\n- Custom aggregation functions can be added\n- Aggregations can be grouped against one or a tuple of columns\n\n**Example:**\n```python\nfrom bigquery_frame.transformations_impl.analyze import __get_test_df\nfrom bigquery_frame.transformations import analyze\n\ndf = __get_test_df()\n\ndf.show()\n# +----+------------+---------------------+--------------------------------------------+\n# | id |       name |               types |                                  evolution |\n# +----+------------+---------------------+--------------------------------------------+\n# |  1 |  Bulbasaur | [\'Grass\', \'Poison\'] | {\'can_evolve\': True, \'evolves_from\': None} |\n# |  2 |    Ivysaur | [\'Grass\', \'Poison\'] |    {\'can_evolve\': True, \'evolves_from\': 1} |\n# |  3 |   Venusaur | [\'Grass\', \'Poison\'] |   {\'can_evolve\': False, \'evolves_from\': 2} |\n# |  4 | Charmander |            [\'Fire\'] | {\'can_evolve\': True, \'evolves_from\': None} |\n# |  5 | Charmeleon |            [\'Fire\'] |    {\'can_evolve\': True, \'evolves_from\': 4} |\n# |  6 |  Charizard |  [\'Fire\', \'Flying\'] |   {\'can_evolve\': False, \'evolves_from\': 5} |\n# |  7 |   Squirtle |           [\'Water\'] | {\'can_evolve\': True, \'evolves_from\': None} |\n# |  8 |  Wartortle |           [\'Water\'] |    {\'can_evolve\': True, \'evolves_from\': 7} |\n# |  9 |  Blastoise |           [\'Water\'] |   {\'can_evolve\': False, \'evolves_from\': 8} |\n# +----+------------+---------------------+--------------------------------------------+\n\nanalyze(df).show()\n# +---------------+------------------------+-------------+-------+----------------+------------+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n# | column_number |            column_name | column_type | count | count_distinct | count_null |       min |       max |                                                                                                                                                                                                                                                                                                                     approx_top_100 |\n# +---------------+------------------------+-------------+-------+----------------+------------+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n# |             0 |                     id |     INTEGER |     9 |              9 |          0 |         1 |         9 |                                                                       [{\'value\': \'1\', \'count\': 1}, {\'value\': \'2\', \'count\': 1}, {\'value\': \'3\', \'count\': 1}, {\'value\': \'4\', \'count\': 1}, {\'value\': \'5\', \'count\': 1}, {\'value\': \'6\', \'count\': 1}, {\'value\': \'7\', \'count\': 1}, {\'value\': \'8\', \'count\': 1}, {\'value\': \'9\', \'count\': 1}] |\n# |             1 |                   name |      STRING |     9 |              9 |          0 | Blastoise | Wartortle | [{\'value\': \'Bulbasaur\', \'count\': 1}, {\'value\': \'Ivysaur\', \'count\': 1}, {\'value\': \'Venusaur\', \'count\': 1}, {\'value\': \'Charmander\', \'count\': 1}, {\'value\': \'Charmeleon\', \'count\': 1}, {\'value\': \'Charizard\', \'count\': 1}, {\'value\': \'Squirtle\', \'count\': 1}, {\'value\': \'Wartortle\', \'count\': 1}, {\'value\': \'Blastoise\', \'count\': 1}] |\n# |             2 |                 types! |      STRING |    13 |              5 |          0 |      Fire |     Water |                                                                                                                                                                  [{\'value\': \'Grass\', \'count\': 3}, {\'value\': \'Poison\', \'count\': 3}, {\'value\': \'Fire\', \'count\': 3}, {\'value\': \'Water\', \'count\': 3}, {\'value\': \'Flying\', \'count\': 1}] |\n# |             3 |   evolution.can_evolve |     BOOLEAN |     9 |              2 |          0 |     false |      true |                                                                                                                                                                                                                                                                    [{\'value\': \'true\', \'count\': 6}, {\'value\': \'false\', \'count\': 3}] |\n# |             4 | evolution.evolves_from |     INTEGER |     9 |              6 |          3 |         1 |         8 |                                                                                                                            [{\'value\': \'NULL\', \'count\': 3}, {\'value\': \'1\', \'count\': 1}, {\'value\': \'2\', \'count\': 1}, {\'value\': \'4\', \'count\': 1}, {\'value\': \'5\', \'count\': 1}, {\'value\': \'7\', \'count\': 1}, {\'value\': \'8\', \'count\': 1}] |\n# +---------------+------------------------+-------------+-------+----------------+------------+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```\n\n\n**Example with custom aggregation and column groups:**\n```python\nfrom bigquery_frame.transformations_impl import analyze_aggs\nfrom bigquery_frame.transformations import analyze\naggs = [\n     analyze_aggs.column_number,\n     analyze_aggs.column_name,\n     analyze_aggs.count,\n     analyze_aggs.count_distinct,\n     analyze_aggs.count_null,\n]\nanalyze(df, group_by="evolution.can_evolve", _aggs=aggs).orderBy(\'group.can_evolve\', \'column_number\').show()\n# +-----------------------+---------------+------------------------+-------+----------------+------------+\n# |                 group | column_number |            column_name | count | count_distinct | count_null |\n# +-----------------------+---------------+------------------------+-------+----------------+------------+\n# | {\'can_evolve\': False} |             0 |                     id |     3 |              3 |          0 |\n# | {\'can_evolve\': False} |             1 |                   name |     3 |              3 |          0 |\n# | {\'can_evolve\': False} |             2 |                 types! |     5 |              5 |          0 |\n# | {\'can_evolve\': False} |             4 | evolution.evolves_from |     3 |              3 |          0 |\n# |  {\'can_evolve\': True} |             0 |                     id |     6 |              6 |          0 |\n# |  {\'can_evolve\': True} |             1 |                   name |     6 |              6 |          0 |\n# |  {\'can_evolve\': True} |             2 |                 types! |     8 |              4 |          0 |\n# |  {\'can_evolve\': True} |             4 | evolution.evolves_from |     6 |              3 |          3 |\n# +-----------------------+---------------+------------------------+-------+----------------+------------+\n```\n\n\n## I want to try this POC, how do I use it ?\n\nJust clone this repository, open PyCharm, and follow the\ninstructions in the [AUTH.md](/AUTH.md) documentation\nto set up your connection to BigQuery. Then, go fiddle\nwith the [demo](/examples/demo.py), or have a look at the [examples](/examples).\n\n\n## How does it work ?\n\nVery simply, by generating SQL queries that are sent to BigQuery.\nYou can get the query by calling the method `DataFrame.compile()`.\n\nFor instance, if we reuse the example from the beginning:\n```\nprint(df2.compile())\n```\n\nThis will print the following SQL query:\n```SQL\nWITH pokedex AS (\n  WITH _default_alias_1 AS (\n    \n        SELECT 1 as id, "Bulbasaur" as name, ["Grass", "Poison"] as types, NULL as other_col\n        UNION ALL\n        SELECT 2 as id, "Ivysaur" as name, ["Grass", "Poison"] as types, NULL as other_col\n    \n  )\n  SELECT \n    id,\n    name,\n    types\n  FROM _default_alias_1\n)\n, _default_alias_3 AS (\n  SELECT * FROM pokedex\n)\n, _default_alias_4 AS (\n  SELECT \n    *,\n    ARRAY_LENGTH(types) AS nb_types\n  FROM _default_alias_3\n)\nSELECT \n  * REPLACE (\n    LOWER(name) AS name\n  )\nFROM _default_alias_4\n```\n\n## Billing\n\nThe examples in this code only use generated data and don\'t ready any "real" table.\nThis means that you won\'t be charged a cent running them.\n\nAlso, even when reading "real" tables, any one-the-fly introspection (such as\ngetting a DataFrame\'s schema), will trigger a query on BigQuery but will read\n0 rows, and will thus be billed 0 cent.\n\n## Known limitations\n\nSince this is a POC, I took some shortcuts and did not try to optimize the query length.\nIn particular, this uses _**a lot**_ of CTEs, and any serious project trying to use it\nmight reach the maximum query length very quickly.\n\nHere is a list of other known limitations, please also see the \n[Further developments](#further-developments) section for a list of missing features.\n\n- `DataFrame.withColumn`: \n  - unlike in Spark, replacing an existing column is  \n    not done automatically, an extra argument `replace=True` must be passed.\n- `DataFrame.createOrReplaceTempView`: \n  - I kept the same name as Spark for consistency, but it does not create an actual view on BigQuery, it just emulates \n    Spark\'s behaviour by using a CTE. Because of this, if you replace a temp view that already exists, the new view\n    can not derive from the old view (while in Spark it is possible). \n- `DataFrame.join`:\n  - When doing a select after a join, table prefixes MUST always be used on column names.\n    For this reason, users SHOULD always make sure the DataFrames they are joining on are properly aliased\n  - When chaining multiple joins, the name of the first DataFrame is not available in the select clause\n\n## Further developments\n\nFunctions not supported yet :\n\n- `DataFrame.groupBy`\n\nAlso, it would be cool to expand this to other SQL engines than BigQuery \n(contributors are welcome ;-) ).\n\n\n## Why did I make this ?\n\nI hope that it will motivate the teams working on BigQuery (or Redshift, \nor Azure Synapse) to propose a real python DataFrame API on top of their \nmassively parallel SQL engines. But not something ugly like this POC,\nthat generates SQL strings, more something like Spark Catalyst, which directly\ngenerates logical plans out of the DataFrame API without passing through the \n"SQL string" step.\n\nAfter starting this POC, I realized Snowflake already understood this and \ndeveloped Snowpark, a Java/Scala (and soon Python) API to run complex workflows\non Snowflake, and [Snowpark\'s DataFrame API](https://docs.snowflake.com/en/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/DataFrame.html)\nwhich was clearly borrowed from [Spark\'s DataFrame (= DataSet[Row]) API](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html)\n(we recognize several key method names: cache, createOrReplaceTempView, \nwhere/filter, collect, toLocalIterator). \n\nI believe such project could open the gate to hundreds of very cool applications.\nFor instance, did you know that, in its early versions at least, Dataiku Shaker \nwas just a GUI that chained transformations on Pandas DataFrame, and later \nSpark DataFrame ? \n\nAnother possible evolution would be to make a DataFrame API capable of speaking\nmultiple SQL dialects. By using it, projects that generate SQL for multiple \nplatforms, like [Malloy](https://github.com/looker-open-source/malloy), could\nall use the same DataFrame abstraction. Adding support for a new SQL platform\nwould immediately allow all the project based on it to support this new platform.\n\n**I would be very interested if someone could make a similar POC with, \nRedShift, Postgres, Azure Synapse, or any other SQL engines \n(aside from Spark-SQL and Snowpark, of course :-p).**\n\n\n## Release Notes\n\n### 0.4.1\n\n- Added `DataFrame.write` feature. It currently supports partitioning, table-level options, and several insertion modes:\n  - `append`: Append contents of this :class:`DataFrame` to existing table.\n  - `overwrite`: Replace destination table with the new data if it already exists.\n  - `error` or `errorifexists`: Throw an exception if destination table already exists.\n  - `ignore`: Silently ignore this operation if destination table already exists.\n- `BigQueryBuilder` now tries to create it\'s own bigquery client if none is passed\n\n### 0.4.0\n\n\n#### New exciting features!\n\nSeveral new features that make working with nested structure easier were added.\n\n- Added `DataFrame.select_nested_columns` and `DataFrame.with_nested_columns`, which \n  make transformation of nested values much easier\n- Added `functions.transform`, useful to transform arrays\n- Added `transformations.normalize_arrays` which automatically sort all arrays in a DataFrame, including\n  arrays of arrays of arrays...\n- Added `transformations.harmonize_dataframes` which takes two DataFrames and transform them into DataFrames\n  with the same schema.\n- Experimental: Added data_diff capabilities, including a DataFrameComparator which can perform a diff\n  between two DataFrames. Extremely useful for non-regression testing.\n- Generated queries are now deterministic, this means that if you re-run the\n  same DataFrame code twice, the exact same query will be sent to BigQuery twice,\n  thus leveraging query caching.\n\n\n#### Other features\n\n- Added automatic retry when BigQuery returns an InternalServerError. We now do 3 tries by default.\n- Added `functions.to_base32` and `functions.to_base64`. \n  `from_base_32` and `from_base_64` will be added later, \n  once a [bug in python-tabulate](https://github.com/astanin/python-tabulate/issues/192) is fixed.\n- Added `Column.__mod__` (e.g. `f.when(c % 2 == 0)`) \n\n\n#### Breaking changes\n\n- Improved typing of `Column.when.otherwise`. Now `functions.when` returns a `WhenColumn`, a\n  special type of `Column` with two extra methods: `when` and `otherwise`.\n- Changed `functions.sort_arrays`\'s signature to make it consistent with `transform`\n\n\n### 0.3.4\n\n#### Features\n\n- added `Column[...]` (`__getitem`) that can be used to access struct or array elements.\n\n#### Bugfixes\n\n- fixed various bugs in transformations.analyze\n  - Was crashing on ARRAY<STRUCT<ARRAY<...>>>\n  - Was crashing on columns of type BYTES\n  - Columns used in group_by were analyzed, which is useless because the group is constant\n\n\n### 0.3.3\n\n#### Breaking changes\n\n- The `bigquery_frame.transformations.analyze` now return one extra column named `column_number`\n\n#### Features\n\n- Add various Column methods: `asc`, `desc`\n- Add various functions methods: `replace`, `array`, \n\n#### Bugfixes\n\n- `Dataframe.sort` now works on aliased columns\n- `Column.alias` now works on sql keywords\n- `functions.eqNullSafe` now never returns NULL\n- fix incorrect line numbering when query was displayed on error\n\n\n### 0.3.2\n\n#### Breaking changes\n\n- `Column` constructor no longer accept `alias` as argument. Use `Column.alias()` instead.\n\n#### Features\n\n- Add various Column methods: `cast`, `~`, `isNull`, `isNotNull`, `eqNullSafe`\n- Add various functions methods: `concat`, `length`, `sort_array`, `substring`\n\n#### Bugfixes\n- Fix broken `Column.when().otherwise().alias()` \n\n### 0.3.1\n\n#### Breaking changes\n- Dropped support for Python 3.6\n- Bumped dependencies versions\n- DataFrame.toPandas() now requires extra permissions by default\n  (the BigQuery ReadSession User role), but downloads data faster.\n\n#### Features\n- Added `functions.cast()` method\n- We now print the whole query in the error message when it fails\n- Added `DataFrame.join()`. This is a first implementation which is a little clumsy.\n\n#### Bugfixes\n- Fix DataFrame deps being lost when using `df.alias()`\n\n',
    'author': 'FurcyPin',
    'author_email': 'None',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/FurcyPin/bigquery-frame',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.7.1,<3.11',
}


setup(**setup_kwargs)
